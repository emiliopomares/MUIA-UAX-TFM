{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c21a0af0-302d-42ea-aad6-d654680f08ba",
   "metadata": {},
   "source": [
    "## CNN VXL-Net - TFM Emilio Pomares Porras MUIA Alfonso X el Sabio, 2023-2024\n",
    "\n",
    "### Sistema de visión artificial para manipulación robótica a tasas interactivas con arquitecturas basadas en UNet: VoxelNet\n",
    "\n",
    "#### Artificial vision system for robotic manipulation at interactive framerates with UNet based architectures: VoxelNet\n",
    "\n",
    "# ETAPA DE CUANTIZACIÓN - QUANTIZATION STAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fc1a20-c8eb-464e-be86-a467a333e696",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ce7a453-97b3-4609-8034-005762411fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.quantization import fuse_modules\n",
    "import torch.quantization\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FixedLocator\n",
    "\n",
    "checkpoint_dir = \"../checkpoints\"\n",
    "checkpoint_fn = \"model_checkpoint_running_mVoxelNet_v4_2_correct_rmisty-meadow-32_tl0.009472012519836426_tm9287_vm9287_256167.16s.pth\" # Your checkpoint here\n",
    "TRAIN_DATASET_PATH = \"/media/emilio/2TBDrive/robovision_train\" # Point to your test data\n",
    "TEST_DATASET_PATH = \"/media/emilio/2TBDrive/robovision_test\" # Point to your test data\n",
    "\n",
    "IMG_SIZE = 256\n",
    "N_CHANNELS = 6\n",
    "BATCH_SIZE = 32 # Let's stick to the classics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "037ced46-d59b-4011-b46a-4c5b071ed86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need the model definition here\n",
    "def copy_inflate(input_tensor):\n",
    "    \"\"\"\n",
    "    Inflates towards the 2nd axis by\n",
    "    producing multiple copies of the\n",
    "    0th-1st axis slice\n",
    "    \"\"\"\n",
    "    # Get the shape of the input tensor\n",
    "    batch_size, C, _, N = input_tensor.shape\n",
    "\n",
    "    # Reshape the input tensor to add a singleton dimension at the end\n",
    "    inflated_tensor = input_tensor.unsqueeze(-1)\n",
    "\n",
    "    # Repeat the singleton dimension N times along the last axis\n",
    "    inflated_tensor = inflated_tensor.expand(-1, -1, -1, -1, N)\n",
    "\n",
    "    return inflated_tensor\n",
    "\n",
    "\n",
    "class DoubleConv2D(nn.Module):\n",
    "    \"\"\"\n",
    "    Double Convolution 2D\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv2D, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, 1, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)     \n",
    "        x = self.bn1(x)     \n",
    "        x = self.relu(x)       \n",
    "        x = self.conv2(x)      \n",
    "        x = self.bn2(x)     \n",
    "        x = self.relu(x)       \n",
    "        return x\n",
    "\n",
    "        \n",
    "class DoubleConv3D(nn.Module):\n",
    "    \"\"\"\n",
    "    Double Convolution 3D\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv3D, self).__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class VXLNet_v4(nn.Module):\n",
    "    \"\"\"\n",
    "    VoxelNet, U-Net inspired network which will\n",
    "    map a 6 channel, stereo RGB image into a\n",
    "    3d 64x64x64 occupation probability map\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, steps=5):\n",
    "        super(VXLNet_v4, self).__init__()\n",
    "\n",
    "        self.encoder = nn.ModuleList()\n",
    "        self.decoder = nn.ModuleList()\n",
    "        self.residual_connections = nn.ModuleList()\n",
    "        self.steps = steps\n",
    "        self.relu = nn.ReLU()\n",
    "        self.mode = \"occupancy\"\n",
    "\n",
    "        features = [2 ** (i+4) for i in range(steps)]\n",
    "\n",
    "        # Encoder\n",
    "        for feature in features:\n",
    "            #print(f\" addinf out feature {feature}\")\n",
    "            self.encoder.append(\n",
    "                    DoubleConv2D(in_channels, feature),\n",
    "            )\n",
    "            in_channels = feature\n",
    "\n",
    "        out_ch = in_channels\n",
    "        #print(\"We start with out_channels: \", out_channels)\n",
    "        \n",
    "        # Decoder\n",
    "        for i in range(1,6):\n",
    "            # Let's try the last layer trick\n",
    "            self.decoder.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose3d(out_ch*2, out_ch, kernel_size=2 if i<5 else 3, stride=2 if i<5 else 3),\n",
    "                    DoubleConv3D(out_ch, out_ch//2)\n",
    "                )\n",
    "            )\n",
    "            out_ch = out_ch//2\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.final_conv_1 = nn.Conv3d(64, 32, kernel_size=1)\n",
    "        self.final_conv_2 = nn.Conv3d(32, 16, kernel_size=1)\n",
    "        self.final_conv_3 = nn.Conv3d(16, 4, kernel_size=1)\n",
    "        self.final_conv_4 = nn.Conv3d(4, out_channels, kernel_size=1)\n",
    "\n",
    "        self.pose_dense_1 = nn.Linear(2 ** (self.steps+4-1), 256)\n",
    "        self.pose_dense_2 = nn.Linear(256, 64)\n",
    "        self.pose_dense_3 = nn.Linear(64, 16)\n",
    "        self.pose_dense_4 = nn.Linear(16, 7)\n",
    "\n",
    "    def set_mode(mode):\n",
    "        freeze_pose = True\n",
    "        if mode=='occupancy':\n",
    "            freeze_pose = True\n",
    "        elif mode=='pose':\n",
    "            freeze_pose = False\n",
    "        else:\n",
    "            raise ValueException(\"Unknown mode\")\n",
    "        # Freeze layer1 and layer2\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = freeze_pose\n",
    "        for param in self.decoder.parameters():\n",
    "            param.requires_grad = freeze_pose\n",
    "        for param in self.residual_connections.parameters():\n",
    "            param.requires_grad = freeze_pose\n",
    "        for param in self.final_conv_1.parameters():\n",
    "            param.requires_grad = freeze_pose\n",
    "        for param in self.final_conv_2.parameters():\n",
    "            param.requires_grad = freeze_pose\n",
    "        for param in self.final_conv_3.parameters():\n",
    "            param.requires_grad = freeze_pose\n",
    "        for param in self.final_conv_4.parameters():\n",
    "            param.requires_grad = freeze_pose\n",
    "        for param in self.pose_dense_1.parameters():\n",
    "            param.requires_grad = not freeze_pose\n",
    "        for param in self.pose_dense_2.parameters():\n",
    "            param.requires_grad = not freeze_pose\n",
    "        for param in self.pose_dense_3.parameters():\n",
    "            param.requires_grad = not freeze_pose\n",
    "        for param in self.pose_dense_4.parameters():\n",
    "            param.requires_grad = not freeze_pose\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        encoder_outputs = []\n",
    "\n",
    "        # Encoder\n",
    "        for module in self.encoder:\n",
    "            x = module(x)\n",
    "            x = self.pool(x)\n",
    "            encoder_outputs.append(x)\n",
    "        \n",
    "        if self.mode == \"pose\":\n",
    "            x_pose = x[:,:,0,0]\n",
    "            x_pose = self.pose_dense_1(x_pose)\n",
    "            x = self.relu(x_pose)\n",
    "            x_pose = self.pose_dense_2(x_pose)\n",
    "            x = self.relu(x_pose)\n",
    "            x_pose = self.pose_dense_3(x_pose)\n",
    "            x = self.relu(x_pose)\n",
    "            x_pose = self.pose_dense_4(x_pose)\n",
    "            return x_pose\n",
    "        \n",
    "        x = x.unsqueeze(-1)\n",
    "        \n",
    "        # Decoder\n",
    "        for i in range(1,6):\n",
    "            residual_connection = encoder_outputs[-i]\n",
    "            inflated_connection = copy_inflate(residual_connection)\n",
    "            x = torch.cat([x, inflated_connection], dim=1)\n",
    "            x = self.decoder[i-1](x)\n",
    "        \n",
    "        x = self.final_conv_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.final_conv_2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.final_conv_3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.final_conv_4(x).squeeze(dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "229c7daf-01e6-4352-9227-6ce092141497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VXLNet_v4(in_channels=6, out_channels=1, steps=8).to(\"cpu\")\n",
    "\n",
    "checkpoint = torch.load(os.path.join(checkpoint_dir, checkpoint_fn))\n",
    "\n",
    "# Load the model's state_dict from the checkpoint\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39d729e-f426-4c03-bc54-93cc454c525f",
   "metadata": {},
   "source": [
    "Let's make sure the model has loaded correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bac6b373-1972-41c0-bc8b-3ffd8d44421c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VXLNet_v4(\n",
       "  (encoder): ModuleList(\n",
       "    (0): DoubleConv2D(\n",
       "      (conv1): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): DoubleConv2D(\n",
       "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): DoubleConv2D(\n",
       "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): DoubleConv2D(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): DoubleConv2D(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): DoubleConv2D(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (6): DoubleConv2D(\n",
       "      (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (7): DoubleConv2D(\n",
       "      (conv1): Conv2d(1024, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): ConvTranspose3d(4096, 2048, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "      (1): DoubleConv3D(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv3d(2048, 1024, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "          (1): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(1024, 1024, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "          (4): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): ConvTranspose3d(2048, 1024, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "      (1): DoubleConv3D(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv3d(1024, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "          (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "          (4): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): ConvTranspose3d(1024, 512, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "      (1): DoubleConv3D(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "          (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "          (4): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): ConvTranspose3d(512, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "      (1): DoubleConv3D(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "          (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "          (4): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): ConvTranspose3d(256, 128, kernel_size=(3, 3, 3), stride=(3, 3, 3))\n",
       "      (1): DoubleConv3D(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "          (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "          (4): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (residual_connections): ModuleList()\n",
       "  (relu): ReLU()\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (final_conv_1): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "  (final_conv_2): Conv3d(32, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "  (final_conv_3): Conv3d(16, 4, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "  (final_conv_4): Conv3d(4, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "  (pose_dense_1): Linear(in_features=2048, out_features=256, bias=True)\n",
       "  (pose_dense_2): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (pose_dense_3): Linear(in_features=64, out_features=16, bias=True)\n",
       "  (pose_dense_4): Linear(in_features=16, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cceec5-5c22-445a-9208-aef80ed1173d",
   "metadata": {},
   "source": [
    "We need at least pytorch 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc336253-8b85-4066-a7b2-c849c871bee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.0.0+cu117\n"
     ]
    }
   ],
   "source": [
    "print(f\"torch version: {torch.__version__}\")\n",
    "assert (int(str(torch.__version__).split(\".\")[0])*100 + int(str(torch.__version__).split(\".\")[1]))>=103, \"Please upgrade to pytorch ^1.3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7988d29b-592d-4496-a367-f420e5f85bd1",
   "metadata": {},
   "source": [
    "We set the model into evaluation mode:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568cbaf8-58e8-4674-9f70-f23e115867a2",
   "metadata": {},
   "source": [
    "## Check quantized model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57f02854-729e-4272-8843-c29525f156a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"\n",
    "    Counts the total number of parameters in a PyTorch model\n",
    "\n",
    "    Parameters:\n",
    "    model (nn.Module): PyTorch model\n",
    "\n",
    "    Returns:\n",
    "    total_params (int): Total number of parameters\n",
    "    total_size (int): Total size of all parameters in bytes\n",
    "    \"\"\"\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    total_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "    \n",
    "    return total_params, total_size, f\"{total_params//(1000000)}M\", f\"{total_size//(1024*1024)}MB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "950c1f34-0a26-41d6-a557-68da2815899e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "\n",
    "def load_target_datapoint(file_path, dataset_path=\"\"):\n",
    "    # Define the format string for reading the binary data\n",
    "    format_string = \"<3f4f\"  # 3 floats (position), 4 floats (quaternion)\n",
    "    # Calculate the size of the bytes for occupation data\n",
    "    occupation_size = 37 * 25 * 18\n",
    "    format_string += str(occupation_size) + \"s\"  # Occupation data\n",
    "\n",
    "    with open(os.path.join(dataset_path, file_path), \"rb\") as file:\n",
    "        # Read the binary data\n",
    "        data = file.read(struct.calcsize(format_string))\n",
    "        # Unpack the binary data according to the format string\n",
    "        unpacked_data = struct.unpack(format_string, data)\n",
    "\n",
    "        # Extract position, rotation, and occupation data\n",
    "        position = unpacked_data[:3]\n",
    "        rotation = unpacked_data[3:7]\n",
    "        # Convert occupation data to array of numbers\n",
    "        occupation = struct.unpack(str(occupation_size) + \"B\", unpacked_data[7])\n",
    "\n",
    "        return position, rotation, occupation\n",
    "    \n",
    "def load_stereo_image(index=0, \n",
    "                      dataset_path=\"./\", \n",
    "                      model_size=(IMG_SIZE, IMG_SIZE), \n",
    "                      l_path=None, \n",
    "                      r_path=None,\n",
    "                      plot=False\n",
    "                     ):\n",
    "    dx = -110\n",
    "    dy = -6\n",
    "    l_file = l_path if l_path is not None else os.path.join(dataset_path, f\"{index}L.png\")\n",
    "    r_file = r_path if r_path is not None else os.path.join(dataset_path, f\"{index}R.png\")\n",
    "    l_img = cv2.cvtColor(cv2.imread(l_file), cv2.COLOR_BGR2RGB)\n",
    "    l_img = cv2.warpAffine(l_img, np.float32([[1, 0, dx], [0, 1, dy]]), (l_img.shape[1], l_img.shape[0]))\n",
    "    r_img = cv2.cvtColor(cv2.imread(r_file), cv2.COLOR_BGR2RGB)\n",
    "    l_img = cv2.resize(l_img[0:714, 0:1170], model_size, interpolation=cv2.INTER_AREA)\n",
    "    r_img = cv2.resize(r_img[0:714, 0:1170], model_size, interpolation=cv2.INTER_AREA)\n",
    "    blended_image = cv2.addWeighted(l_img, 0.5, r_img, 0.5, 0)\n",
    "    \n",
    "    if plot:\n",
    "        # Plot the blended image\n",
    "        plt.title(f\"sample {index} ({model_size[0]}x{model_size[0]} 3+3 channel, aspect corrected)\")\n",
    "        plt.imshow(blended_image, aspect=1/1.4)\n",
    "        \n",
    "    return torch.tensor(l_img/255.0), torch.tensor(r_img/255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "579d10cb-f133-4193-ab60-835049adb7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3d(data, aspect='equal', double_axis=True, ax=None):\n",
    "    \"\"\"\n",
    "    Plots a 3D grid of scalar-valued voxels.\n",
    "\n",
    "    Parameters:\n",
    "    data (numpy.array): data (len(data.shape) should be 3)\n",
    "    aspect (str): Aspect ratio for the plot (default is 'equal')\n",
    "    double_axis (bool): Whether to scale the tick labels by a factor of 2\n",
    "    ax (matplotlib.axes._subplots.Axes3DSubplot, optional): The axis to plot on. \n",
    "                                                           If None, a new figure and axis are created.\n",
    "    \"\"\"\n",
    "\n",
    "    def explode(data):\n",
    "        size = np.array(data.shape) * 2\n",
    "        data_e = np.zeros(size - 1, dtype=data.dtype)\n",
    "        data_e[::2, ::2, ::2] = data\n",
    "        return data_e\n",
    "\n",
    "    def explode_2(data):\n",
    "        size = np.array(data.shape) * 2\n",
    "        size[3] = data.shape[3]\n",
    "        data_e = np.zeros((size[0] - 1, size[1] - 1, size[2] - 1, data.shape[3]), dtype=data.dtype)\n",
    "        data_e[::2, ::2, ::2, :] = data\n",
    "        return data_e\n",
    "\n",
    "    filled = data\n",
    "    colors = np.zeros(filled.shape + (4,))\n",
    "    colors[..., 3] = filled  # Use tensor values for the alpha channel\n",
    "    colors[..., :3] = plt.cm.viridis(filled)[..., :3]  # Assign colors using the viridis colormap\n",
    "\n",
    "    # Upscale the above voxel image, leaving gaps\n",
    "    filled_2 = explode(filled)\n",
    "    colors_2 = explode_2(colors)\n",
    "\n",
    "    # Shrink the gaps\n",
    "    x, y, z = np.indices(np.array(filled_2.shape) + 1).astype(float) // 2\n",
    "    x[0::2, :, :] += 0.05\n",
    "    y[:, 0::2, :] += 0.05\n",
    "    z[:, :, 0::2] += 0.05\n",
    "    x[1::2, :, :] += 0.95\n",
    "    y[:, 1::2, :] += 0.95\n",
    "    z[:, :, 1::2] += 0.95\n",
    "\n",
    "    # Create a new figure and axis only if ax is None\n",
    "    if ax is None:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "    ax.voxels(x, y, z, filled_2, facecolors=colors_2)\n",
    "    ax.set_aspect(aspect)\n",
    "\n",
    "    if double_axis:\n",
    "        # Scale the tick labels by a factor of 2\n",
    "        xticks = ax.get_xticks()\n",
    "        yticks = ax.get_yticks()\n",
    "        zticks = ax.get_zticks()\n",
    "    \n",
    "        # Set the tick positions (using FixedLocator)\n",
    "        ax.xaxis.set_major_locator(FixedLocator(xticks))\n",
    "        ax.yaxis.set_major_locator(FixedLocator(yticks))\n",
    "        ax.zaxis.set_major_locator(FixedLocator(zticks))\n",
    "    \n",
    "        # Now set the tick labels\n",
    "        ax.set_xticklabels(np.round(xticks * 2, 2))\n",
    "        ax.set_yticklabels(np.round(yticks * 2, 2))\n",
    "        ax.set_zticklabels(np.round(zticks * 2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1acd637b-0505-43a2-b623-3cd68ef7d957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_input(l, r, permute=True):\n",
    "    \"\"\"Creates a tensor input datapoint to be fed into the model\n",
    "    from l and r images\n",
    "    Parameters:\n",
    "    - l: left image\n",
    "    - r: right image\n",
    "    - permute (bool): apply permutation\n",
    "    Returns:\n",
    "    torch.tensor\n",
    "    \"\"\"\n",
    "    if permute:\n",
    "        assert l.shape == (IMG_SIZE, IMG_SIZE, N_CHANNELS//2)\n",
    "        assert r.shape == (IMG_SIZE, IMG_SIZE, N_CHANNELS//2)\n",
    "        lr = torch.cat([l, r], dim=2).permute(2,0,1).float()\n",
    "    else:\n",
    "        assert l.shape == (N_CHANNELS//2, IMG_SIZE, IMG_SIZE)\n",
    "        assert r.shape == (N_CHANNELS//2, IMG_SIZE, IMG_SIZE)\n",
    "        lr = torch.cat([l, r], dim=0).float()\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b7fd702-5dd8-4c0f-b7a5-01273f558581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padded_size(data, N):\n",
    "    \"\"\"\n",
    "    Calculates padding sizes for F.pad so that each dimension is a multiple of N\n",
    "    \"\"\"\n",
    "    n_dim = len(data.shape)\n",
    "    dims = []\n",
    "    for dim in range(n_dim):\n",
    "        l = data.shape[dim]\n",
    "        needed = ((N-(l-(l//N)*N))%N)\n",
    "        needed_low = needed//2\n",
    "        needed_high = needed-needed_low\n",
    "        dims.append(needed_low)\n",
    "        dims.append(needed_high)\n",
    "    return tuple(dims[::-1])\n",
    "\n",
    "def permute_target_tensor(t):\n",
    "    return torch.flip(t, dims=[2]).permute(2, 0, 1)\n",
    "\n",
    "def unpermute_target_tensor(t):\n",
    "    return torch.flip(t.permute(1, 2, 0), dims=[2])\n",
    "\n",
    "def crop_output(t):\n",
    "    return t[6:6+37, 12:12+25, 15:15+18]\n",
    "\n",
    "def expand_output(t):\n",
    "    return F.pad(t, get_padded_size(np.zeros(t.shape), 48))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8da5101-31cc-4e94-90ac-610744a17770",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataLoader(Dataset):\n",
    "    \"\"\"Class to load data from files in disk and convert\n",
    "    them to tensors on the fly\"\"\"\n",
    "    def __init__(self, data_dir, transform=None, additional_param=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.l_img_list = []\n",
    "        self.r_img_list = []\n",
    "        self.gt_list = []\n",
    "        self.additional_param = additional_param\n",
    "        for file in sorted(os.listdir(data_dir)):\n",
    "            if file.endswith(\"L.png\"):\n",
    "                self.l_img_list.append(os.path.join(data_dir, file))\n",
    "            elif file.endswith(\"R.png\"):\n",
    "                self.r_img_list.append(os.path.join(data_dir, file))\n",
    "            elif file.endswith(\"T.bin\"):\n",
    "                self.gt_list.append(os.path.join(data_dir, file))   \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.gt_list)\n",
    "\n",
    "    def __getitem__(self, idx):        \n",
    "        l, r = load_stereo_image(l_path=self.l_img_list[idx], r_path=self.r_img_list[idx]);\n",
    "        t, q, o = load_target_datapoint(self.gt_list[idx])\n",
    "\n",
    "        # Apply transformations if specified\n",
    "        if self.transform:\n",
    "            l = self.transform(l)\n",
    "            r = self.transform(r)\n",
    "\n",
    "        l = l.permute(2, 0, 1)\n",
    "        r = r.permute(2, 0, 1)\n",
    "\n",
    "        augment_contrast=self.additional_param['contrast'] if (self.additional_param is not None and 'contrast' in self.additional_param) else 0\n",
    "        augment_saturation=self.additional_param['saturation'] if (self.additional_param is not None and 'saturation' in self.additional_param) else 0\n",
    "        augment_brightness=self.additional_param['brightness'] if (self.additional_param is not None and 'brightness' in self.additional_param) else 0\n",
    "        augment_hue=self.additional_param['hue'] if (self.additional_param is not None and 'hue' in self.additional_param) else 0\n",
    "        augment_noise=self.additional_param['noise'] if (self.additional_param is not None and 'noise' in self.additional_param) else 0\n",
    "\n",
    "        # Augment each image separately, as l and r cameras are independent\n",
    "\n",
    "        l = transforms.functional.adjust_brightness(l, 1 + (random.random() - 0.5) * 2 * augment_brightness)\n",
    "        l = transforms.functional.adjust_contrast(l, 1 + (random.random() - 0.5) * 2 * augment_contrast)\n",
    "        l = transforms.functional.adjust_saturation(l, 1 + (random.random() - 0.5) * 2 * augment_saturation)\n",
    "        l = transforms.functional.adjust_hue(l, (random.random() - 0.5) * 2 * augment_hue)\n",
    "        noise = np.random.normal(0, augment_noise, l.shape).astype(np.float32)\n",
    "        l = l + noise\n",
    "        l = torch.clip(l, 0, 1)\n",
    "\n",
    "        r = transforms.functional.adjust_brightness(r, 1 + (random.random() - 0.5) * 2 * augment_brightness)\n",
    "        r = transforms.functional.adjust_contrast(r, 1 + (random.random() - 0.5) * 2 * augment_contrast)\n",
    "        r = transforms.functional.adjust_saturation(r, 1 + (random.random() - 0.5) * 2 * augment_saturation)\n",
    "        r = transforms.functional.adjust_hue(r, (random.random() - 0.5) * 2 * augment_hue)\n",
    "        noise = np.random.normal(0, augment_noise, r.shape).astype(np.float32)\n",
    "        r = r + noise\n",
    "        r = torch.clip(r, 0, 1)\n",
    "            \n",
    "        X = make_model_input(l, r, permute=False)\n",
    "            \n",
    "        occupation = torch.tensor(np.array(o, dtype='float32').reshape(37,25,18))\n",
    "        occupation = expand_output(occupation)\n",
    "        y = permute_target_tensor(occupation)\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9d50614-f75c-4092-acc3-291b236de091",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_params = {\n",
    "    'contrast': 0.2,\n",
    "    'brightness': 0.2,\n",
    "    'saturation': 0.2,\n",
    "    'hue': 0.2,\n",
    "    'noise': 0.02\n",
    "}\n",
    "\n",
    "train_dataloader = CustomDataLoader(TRAIN_DATASET_PATH, additional_param=aug_params)\n",
    "train_loader = DataLoader(train_dataloader, batch_size=32)\n",
    "\n",
    "test_dataloader = CustomDataLoader(TEST_DATASET_PATH, additional_param=aug_params)\n",
    "test_loader = DataLoader(test_dataloader, batch_size=32)\n",
    "\n",
    "test_batch = None\n",
    "for batch in test_loader:\n",
    "    test_batch = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b2400d-6fd1-4140-8dc9-d07ffeb29810",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "torch.set_num_threads(24)\n",
    "th = nn.Threshold(0.5, 0)\n",
    "with torch.no_grad():\n",
    "    idx = 2\n",
    "    model.eval()\n",
    "    for batch in test_loader:\n",
    "        X = batch[0].to(device)\n",
    "        y = batch[1].to(device)\n",
    "        X_gpu = X.to(\"cuda:0\", copy=True)\n",
    "        print(X.shape)\n",
    "        print(y.shape)\n",
    "        plt.imshow(X[idx,0])\n",
    "        #start_time_quant = time.perf_counter() # PERF\n",
    "        #pred = model_quantized(X_gpu[idx:idx+1])\n",
    "        #end_time_quant = time.perf_counter() # PERF\n",
    "        #execution_time_quant = end_time_quant - start_time_quant # PERF\n",
    "        start_time = time.perf_counter() # PERF\n",
    "        pred = model(X[idx:idx+1])\n",
    "        end_time = time.perf_counter() # PERF\n",
    "        execution_time = end_time - start_time # PERF\n",
    "        #print(f\"Original model inference time: {execution_time*1000}ms  -vs-  Quantized model inference time: {execution_time_quant*1000}ms\")\n",
    "        occ = crop_output(unpermute_target_tensor(torch.sigmoid(pred)[0]))\n",
    "        occ = (occ-torch.min(occ)) / (torch.max(occ)-torch.min(occ))\n",
    "        occ = (occ > 0.25) * occ # th\n",
    "        print(occ.shape)\n",
    "        \n",
    "        # Create a figure with 2 subplots with 3d projection arranged horizontally\n",
    "        fig, axs = plt.subplots(1, 2, subplot_kw={'projection': '3d'}, figsize=(10, 5))\n",
    "        \n",
    "        plot_3d(occ.detach().numpy(), ax=axs[0])\n",
    "        axs[0].set_title(\"Prediction\")\n",
    "        \n",
    "        plot_3d(crop_output(unpermute_target_tensor(y[idx])).detach().numpy(), ax=axs[1])\n",
    "        axs[1].set_title(\"Ground truth\")\n",
    "        \n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50ef54b5-a5be-4e0c-a680-bf12f616911b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(279230528, 1116922112, '279M', '1065MB')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3604af2a-bc28-4ec7-bd12-e88af774138f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90017664, 360070656, '90M', '343MB')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model_quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa149388-0e24-483b-9179-19e08507bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_datatypes(model):\n",
    "    # Print data types of model parameters\n",
    "    print(\"Parameters data types:\")\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f\"{name}: {param.dtype}\")\n",
    "\n",
    "    # Print data types of model buffers\n",
    "    print(\"\\nBuffers data types:\")\n",
    "    for name, buffer in model.named_buffers():\n",
    "        print(f\"{name}: {buffer.dtype}\")\n",
    "\n",
    "print_datatypes(model_int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8d285946-264a-47ab-b86a-246bb292b31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model size: 1116.92 MB\n",
      "Quantized model size: 1114.75 MB\n"
     ]
    }
   ],
   "source": [
    "def model_size(model):\n",
    "    return sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "\n",
    "original_size = model_size(model)\n",
    "quantized_size = model_size(model_int8)\n",
    "\n",
    "print(f\"Original model size: {original_size / 1e6:.2f} MB\")\n",
    "print(f\"Quantized model size: {quantized_size / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747f223d-d504-4f28-81c4-dca1ad732c30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39d6be0-c899-4113-ab54-d89133327202",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30b2bf0-cc7a-4c3c-bb04-cd8751102667",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b566ffb-fa92-42cd-89e7-9dd3936ce205",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d053e92d-a6dd-46d1-9201-897b7816bd2a",
   "metadata": {},
   "source": [
    "We then apply dynamic quantization. We could resort to static quantization if the model was not performant enough in our target platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb9294b-09b5-4c50-9471-3dc4937729d4",
   "metadata": {},
   "source": [
    "Save the quantized model to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb758b17-d9b7-4837-8393-0149b120b62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_model(model):\n",
    "    \"\"\"\n",
    "    Strips a PyTorch model of all gradient information and moves it to CPU.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The PyTorch model to be stripped.\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module: The stripped model moved to CPU.\n",
    "    \"\"\"\n",
    "    # Step 1: Detach the parameters and buffers from the computational graph\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for buffer in model.buffers():\n",
    "        buffer.requires_grad = False\n",
    "\n",
    "    # Step 2: Move the model to CPU (if necessary)\n",
    "    model_cpu = model.cpu()\n",
    "\n",
    "    # Return the stripped model\n",
    "    return model_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70338c65-8ee9-4ca7-ba9e-6c07fa4ffaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = strip_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34da3008-ca0d-4bff-95a3-72ea325aee4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# PERFORM STATIC QUANTIZATION W/CALIBRATION\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import copy\n",
    "\n",
    "backend = \"qnnpack\"  # running on a x86 CPU. Use \"qnnpack\" if running on ARM.\n",
    "\n",
    "## FX GRAPH\n",
    "from torch.quantization import quantize_fx\n",
    "m2 = copy.deepcopy(model)\n",
    "m2.eval()\n",
    "qconfig_dict = {\"\": torch.quantization.get_default_qconfig(backend)}\n",
    "# Prepare\n",
    "model_prepared = quantize_fx.prepare_fx(m2, qconfig_dict, test_batch[0])\n",
    "# Calibrate - Use representative (validation) data.\n",
    "with torch.inference_mode():\n",
    "  n=0\n",
    "  for batch in test_loader:\n",
    "      x = batch[0]\n",
    "      print(\".\")\n",
    "      model_prepared(x)\n",
    "      n+=1\n",
    "      if n==10:\n",
    "          break\n",
    "# quantize\n",
    "model_quantized = quantize_fx.convert_fx(model_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac150938-9876-467d-8b43-7e351f547e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9390ea9-6dcc-4fda-82da-8fefa6c69a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model size: 1116.92 MB\n",
      "Quantized model size: 360.07 MB\n"
     ]
    }
   ],
   "source": [
    "def model_size(model):\n",
    "    return sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "\n",
    "original_size = model_size(model)\n",
    "quantized_size = model_size(model_quantized)\n",
    "\n",
    "print(f\"Original model size: {original_size / 1e6:.2f} MB\")\n",
    "print(f\"Quantized model size: {quantized_size / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81fdedbd-5c5f-4952-9815-a60ac84778f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_quantized.state_dict(), os.path.join(checkpoint_dir, \"quantized-int8.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c21c668f-273e-4ff4-a88e-00877bc3ea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_quantized = strip_model(model_quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b837086f-9fa9-4c68-8c11-8740c778939e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "model_quantized.to(device)\n",
    "data = batch[0][0:32].to(device, copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a94730d4-1f74-47ad-a130-eb7efefbaa53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385.55799424648285 ms\n",
      "385.27579605579376 ms\n",
      "384.6462592482567 ms\n",
      "385.2265179157257 ms\n",
      "400.11780709028244 ms\n",
      "406.7502021789551 ms\n",
      "394.68200504779816 ms\n",
      "391.3705348968506 ms\n",
      "396.3255137205124 ms\n",
      "384.2552602291107 ms\n",
      "398.4268978238106 ms\n",
      "386.35334372520447 ms\n",
      "385.34457236528397 ms\n",
      "400.42661875486374 ms\n",
      "422.68939316272736 ms\n",
      "389.60421830415726 ms\n",
      "394.1529169678688 ms\n",
      "411.53597831726074 ms\n",
      "389.41898196935654 ms\n",
      "427.42322385311127 ms\n",
      "Average time per frame: 395.9824610501528\n"
     ]
    }
   ],
   "source": [
    "m = 20\n",
    "om = m\n",
    "tot = 0\n",
    "for b in test_loader:\n",
    "    start_time = time.time()\n",
    "    model(b[0].to(device))\n",
    "    print(f\"{(time.time()-start_time)*1000/32} ms\")\n",
    "    tot += (time.time()-start_time)*1000/32\n",
    "    m -= 1\n",
    "    if m == 0:\n",
    "        break\n",
    "print(f\"Average time per frame: {tot/om}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "be28f3d5-67d7-44d9-8232-a1f35f05bb6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Inference tensors cannot be saved for backward. To work around you can make a clone to get a normal tensor and use it in autograd.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[197], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mstart_time)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1000\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/StableVITON/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[186], line 151\u001b[0m, in \u001b[0;36mVoxelNet_v4.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# Encoder\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder:\n\u001b[0;32m--> 151\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(x)\n\u001b[1;32m    153\u001b[0m     encoder_outputs\u001b[38;5;241m.\u001b[39mappend(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/StableVITON/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[186], line 33\u001b[0m, in \u001b[0;36mDoubleConv2D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 33\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m     \n\u001b[1;32m     34\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(x)     \n\u001b[1;32m     35\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)       \n",
      "File \u001b[0;32m~/miniconda3/envs/StableVITON/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/StableVITON/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/StableVITON/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Inference tensors cannot be saved for backward. To work around you can make a clone to get a normal tensor and use it in autograd."
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model(batch[0][0:1])\n",
    "print(f\"{(time.time()-start_time)*1000} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "55b39b06-cc91-4764-877d-56587357c123",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Inference tensors cannot be saved for backward. To work around you can make a clone to get a normal tensor and use it in autograd.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[200], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/StableVITON/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[186], line 151\u001b[0m, in \u001b[0;36mVoxelNet_v4.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# Encoder\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder:\n\u001b[0;32m--> 151\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(x)\n\u001b[1;32m    153\u001b[0m     encoder_outputs\u001b[38;5;241m.\u001b[39mappend(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/StableVITON/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[186], line 33\u001b[0m, in \u001b[0;36mDoubleConv2D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 33\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m     \n\u001b[1;32m     34\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(x)     \n\u001b[1;32m     35\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)       \n",
      "File \u001b[0;32m~/miniconda3/envs/StableVITON/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/StableVITON/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/StableVITON/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Inference tensors cannot be saved for backward. To work around you can make a clone to get a normal tensor and use it in autograd."
     ]
    }
   ],
   "source": [
    "model(batch[0][0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f31432f-af2c-43df-b8d9-1817684457fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emilio/miniconda3/envs/StableVITON/lib/python3.10/site-packages/torch/_utils.py:314: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  device=storage.device,\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ConvReLU2d' object has no attribute '_modules'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_quantized \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquantized-int8.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/StableVITON/lib/python3.10/site-packages/torch/serialization.py:809\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    808\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 809\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[1;32m    811\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/StableVITON/lib/python3.10/site-packages/torch/serialization.py:1172\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1170\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1171\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1172\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1174\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/StableVITON/lib/python3.10/site-packages/torch/fx/graph_module.py:109\u001b[0m, in \u001b[0;36mreduce_graph_module\u001b[0;34m(body, import_block)\u001b[0m\n\u001b[1;32m    107\u001b[0m fn_src \u001b[38;5;241m=\u001b[39m body\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_code\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m body[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    108\u001b[0m forward \u001b[38;5;241m=\u001b[39m _forward_from_src(import_block \u001b[38;5;241m+\u001b[39m fn_src, {})\n\u001b[0;32m--> 109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_deserialize_graph_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/StableVITON/lib/python3.10/site-packages/torch/fx/graph_module.py:168\u001b[0m, in \u001b[0;36m_deserialize_graph_module\u001b[0;34m(forward, body)\u001b[0m\n\u001b[1;32m    165\u001b[0m com \u001b[38;5;241m=\u001b[39m CodeOnlyModule(body)\n\u001b[1;32m    167\u001b[0m tracer_extras \u001b[38;5;241m=\u001b[39m body\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_tracer_extras\u001b[39m\u001b[38;5;124m'\u001b[39m, {})\n\u001b[0;32m--> 168\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[43mKeepModules\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtracer_extras\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# Manually set Tracer class on the reconstructed Graph, to avoid\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# referencing the private local subclass KeepModules.\u001b[39;00m\n\u001b[1;32m    172\u001b[0m graph\u001b[38;5;241m.\u001b[39m_tracer_cls \u001b[38;5;241m=\u001b[39m tracer_cls\n",
      "File \u001b[0;32m~/miniconda3/envs/StableVITON/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:705\u001b[0m, in \u001b[0;36mTracer.trace\u001b[0;34m(self, root, concrete_args)\u001b[0m\n\u001b[1;32m    703\u001b[0m     fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(root), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraced_func_name)\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_module_name \u001b[38;5;241m=\u001b[39m root\u001b[38;5;241m.\u001b[39m_get_name()\n\u001b[0;32m--> 705\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmodule_paths \u001b[38;5;241m=\u001b[39m {mod: name \u001b[38;5;28;01mfor\u001b[39;00m name, mod \u001b[38;5;129;01min\u001b[39;00m root\u001b[38;5;241m.\u001b[39mnamed_modules()}\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule()\n",
      "File \u001b[0;32m~/miniconda3/envs/StableVITON/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:705\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    703\u001b[0m     fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(root), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraced_func_name)\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_module_name \u001b[38;5;241m=\u001b[39m root\u001b[38;5;241m.\u001b[39m_get_name()\n\u001b[0;32m--> 705\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmodule_paths \u001b[38;5;241m=\u001b[39m {mod: name \u001b[38;5;28;01mfor\u001b[39;00m name, mod \u001b[38;5;129;01min\u001b[39;00m root\u001b[38;5;241m.\u001b[39mnamed_modules()}\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule()\n",
      "File \u001b[0;32m~/miniconda3/envs/StableVITON/lib/python3.10/site-packages/torch/nn/modules/module.py:2266\u001b[0m, in \u001b[0;36mModule.named_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   2264\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   2265\u001b[0m submodule_prefix \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m name\n\u001b[0;32m-> 2266\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mnamed_modules(memo, submodule_prefix, remove_duplicate):\n\u001b[1;32m   2267\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m m\n",
      "File \u001b[0;32m~/miniconda3/envs/StableVITON/lib/python3.10/site-packages/torch/nn/modules/module.py:2266\u001b[0m, in \u001b[0;36mModule.named_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   2264\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   2265\u001b[0m submodule_prefix \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m name\n\u001b[0;32m-> 2266\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mnamed_modules(memo, submodule_prefix, remove_duplicate):\n\u001b[1;32m   2267\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m m\n",
      "File \u001b[0;32m~/miniconda3/envs/StableVITON/lib/python3.10/site-packages/torch/nn/modules/module.py:2266\u001b[0m, in \u001b[0;36mModule.named_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   2264\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   2265\u001b[0m submodule_prefix \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m name\n\u001b[0;32m-> 2266\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mnamed_modules(memo, submodule_prefix, remove_duplicate):\n\u001b[1;32m   2267\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m m\n",
      "File \u001b[0;32m~/miniconda3/envs/StableVITON/lib/python3.10/site-packages/torch/nn/modules/module.py:2262\u001b[0m, in \u001b[0;36mModule.named_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   2260\u001b[0m     memo\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   2261\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m prefix, \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m-> 2262\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_modules\u001b[49m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2263\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2264\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/StableVITON/lib/python3.10/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ConvReLU2d' object has no attribute '_modules'"
     ]
    }
   ],
   "source": [
    "model_quantized = torch.load(os.path.join(checkpoint_dir, \"quantized-int8.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "2771f14c-294d-43c3-85f6-aea134e1fa9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'VoxelNet_v4' object has no attribute 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[211], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/StableVITON/lib/python3.10/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'VoxelNet_v4' object has no attribute 'device'"
     ]
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "88df5f00-5a15-42a2-9c2c-e553b0603aa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model_quantized.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "bb438527-ce99-4faa-ab38-a28b6445ecdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphModule(\n",
       "  (encoder): Module(\n",
       "    (0): Module(\n",
       "      (conv1): QuantizedConvReLU2d(6, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.045254532247781754, zero_point=0, padding=(1, 1))\n",
       "      (conv2): QuantizedConvReLU2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.04280085489153862, zero_point=0, padding=(1, 1))\n",
       "    )\n",
       "    (1): Module(\n",
       "      (conv1): QuantizedConvReLU2d(16, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.04000698775053024, zero_point=0, padding=(1, 1))\n",
       "      (conv2): QuantizedConvReLU2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.04844626784324646, zero_point=0, padding=(1, 1))\n",
       "    )\n",
       "    (2): Module(\n",
       "      (conv1): QuantizedConvReLU2d(32, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.033424392342567444, zero_point=0, padding=(1, 1))\n",
       "      (conv2): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.024270353838801384, zero_point=0, padding=(1, 1))\n",
       "    )\n",
       "    (3): Module(\n",
       "      (conv1): QuantizedConvReLU2d(64, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.01917353644967079, zero_point=0, padding=(1, 1))\n",
       "      (conv2): QuantizedConvReLU2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.015821119770407677, zero_point=0, padding=(1, 1))\n",
       "    )\n",
       "    (4): Module(\n",
       "      (conv1): QuantizedConvReLU2d(128, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.01165704894810915, zero_point=0, padding=(1, 1))\n",
       "      (conv2): QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.01547111663967371, zero_point=0, padding=(1, 1))\n",
       "    )\n",
       "    (5): Module(\n",
       "      (conv1): QuantizedConvReLU2d(256, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.008695892058312893, zero_point=0, padding=(1, 1))\n",
       "      (conv2): QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.012135909870266914, zero_point=0, padding=(1, 1))\n",
       "    )\n",
       "    (6): Module(\n",
       "      (conv1): QuantizedConvReLU2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), scale=0.008603567257523537, zero_point=0, padding=(1, 1))\n",
       "      (conv2): QuantizedConvReLU2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), scale=0.005908142309635878, zero_point=0, padding=(1, 1))\n",
       "    )\n",
       "    (7): Module(\n",
       "      (conv1): QuantizedConvReLU2d(1024, 2048, kernel_size=(3, 3), stride=(1, 1), scale=0.0027883986476808786, zero_point=0, padding=(1, 1))\n",
       "      (conv2): QuantizedConvReLU2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), scale=0.0020782658830285072, zero_point=0, padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (decoder): Module(\n",
       "    (0): Module(\n",
       "      (0): QuantizedConvTranspose3d(Reference)(4096, 2048, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "      (1): Module(\n",
       "        (double_conv): Module(\n",
       "          (0): QuantizedConvReLU3d(2048, 1024, kernel_size=(3, 3, 3), stride=(1, 1, 1), scale=0.0006429622299037874, zero_point=0, padding=(1, 1, 1))\n",
       "          (3): QuantizedConvReLU3d(1024, 1024, kernel_size=(3, 3, 3), stride=(1, 1, 1), scale=0.00590717326849699, zero_point=0, padding=(1, 1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): Module(\n",
       "      (0): QuantizedConvTranspose3d(Reference)(2048, 1024, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "      (1): Module(\n",
       "        (double_conv): Module(\n",
       "          (0): QuantizedConvReLU3d(1024, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), scale=0.007073620334267616, zero_point=0, padding=(1, 1, 1))\n",
       "          (3): QuantizedConvReLU3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), scale=0.011819323524832726, zero_point=0, padding=(1, 1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): Module(\n",
       "      (0): QuantizedConvTranspose3d(Reference)(1024, 512, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "      (1): Module(\n",
       "        (double_conv): Module(\n",
       "          (0): QuantizedConvReLU3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), scale=0.015120234340429306, zero_point=0, padding=(1, 1, 1))\n",
       "          (3): QuantizedConvReLU3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), scale=0.023877719417214394, zero_point=0, padding=(1, 1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): Module(\n",
       "      (0): QuantizedConvTranspose3d(Reference)(512, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "      (1): Module(\n",
       "        (double_conv): Module(\n",
       "          (0): QuantizedConvReLU3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), scale=0.02190156653523445, zero_point=0, padding=(1, 1, 1))\n",
       "          (3): QuantizedConvReLU3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), scale=0.020463518798351288, zero_point=0, padding=(1, 1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): Module(\n",
       "      (0): QuantizedConvTranspose3d(Reference)(256, 128, kernel_size=(3, 3, 3), stride=(3, 3, 3))\n",
       "      (1): Module(\n",
       "        (double_conv): Module(\n",
       "          (0): QuantizedConvReLU3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), scale=0.016524989157915115, zero_point=0, padding=(1, 1, 1))\n",
       "          (3): QuantizedConvReLU3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), scale=0.01392851397395134, zero_point=0, padding=(1, 1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_conv_1): QuantizedConvReLU3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), scale=0.033042993396520615, zero_point=0)\n",
       "  (final_conv_2): QuantizedConvReLU3d(32, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), scale=0.06361857801675797, zero_point=0)\n",
       "  (final_conv_3): QuantizedConvReLU3d(16, 4, kernel_size=(1, 1, 1), stride=(1, 1, 1), scale=0.11641238629817963, zero_point=0)\n",
       "  (final_conv_4): QuantizedConv3d(4, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1), scale=0.07015695422887802, zero_point=250)\n",
       ")"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_quantized.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0ea64421-63be-42b0-a8be-95b410b80ec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6f534c35-8c04-42d3-9362-9d0e5ec7da6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.217274740226856"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(267)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "139c8766-463f-4fdc-bd07-ee55240c3914",
   "metadata": {},
   "outputs": [],
   "source": [
    "E = 0.0006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e4f65c7a-7b2f-43f6-818b-ba1bf136d633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en orientación: 1.4034542422206169 grados\n"
     ]
    }
   ],
   "source": [
    "print(f\"Error en orientación: {np.sqrt(E)*360/(2*np.pi)} grados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f7e62136-5cb7-40c7-9195-7f6010ce2f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en posicion train: 1.5394804318340651 cm\n"
     ]
    }
   ],
   "source": [
    "E = 237\n",
    "print(f\"Error en posicion train: {np.sqrt(E)/10} cm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2a0ffa83-4236-487e-a50e-4fcd31588ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en posicion test: 1.5811388300841895 cm\n"
     ]
    }
   ],
   "source": [
    "E = 250\n",
    "print(f\"Error en posicion test: {np.sqrt(E)/10} cm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63b3a11-3a60-47b8-8274-a46df5afc4e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
